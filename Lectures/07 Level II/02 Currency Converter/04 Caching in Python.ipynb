{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/caching.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is a powerful technique that allows you to store the results of expensive function calls and return the cached result when the same inputs occur again. This can significantly improve the efficiency of your programs by avoiding redundant computations. In this lecture, we will explore different caching strategies provided by Python's `functools` module, such as `@cache` and `@lru_cache`, and discuss how to implement a cache with a time-to-live (TTL) feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Understanding Caching](#toc1_)    \n",
    "  - [The Basics of Caching](#toc1_1_)    \n",
    "- [Using `@cache`](#toc2_)    \n",
    "  - [Example of `@cache`](#toc2_1_)    \n",
    "- [Using `@lru_cache`](#toc3_)    \n",
    "  - [Configuring `@lru_cache`](#toc3_1_)    \n",
    "- [Implementing Cache with TTL](#toc4_)    \n",
    "  - [Using `cachetools` for TTL](#toc4_1_)    \n",
    "  - [Building a Custom TTL Cache](#toc4_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Understanding Caching](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching can be particularly beneficial in scenarios where a function is deterministic (the same inputs always produce the same output) and the function is called repeatedly with the same arguments. By storing the results of these function calls, we can reduce the computational overhead and speed up our programs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[The Basics of Caching](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, caching involves storing the results of a function call along with its arguments. When the function is called, the cache is checked to see if the result for those arguments is already available. If it is, the cached result is returned immediately, skipping the actual computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Using `@cache`](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduced in Python 3.9, the `@cache` decorator is a simple way to add caching to a function without any configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_'></a>[Example of `@cache`](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "@cache\n",
    "def fibonacci(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci(n - 1) + fibonacci(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first call will compute the result\n",
    "fibonacci(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsequent calls with the same argument will fetch the result from the cache\n",
    "fibonacci(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Using `@lru_cache`](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more control over the caching behavior, `@lru_cache` can be used. It implements a least-recently-used cache, which evicts the least recently accessed items when the cache is full.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_'></a>[Configuring `@lru_cache`](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the maximum size of the cache using the `maxsize` argument. Additionally, you can obtain cache statistics and clear the cache when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=128)\n",
    "def fibonacci(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci(n - 1) + fibonacci(n - 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=0, misses=0, maxsize=128, currsize=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cache_info method provides statistics about cache usage\n",
    "fibonacci.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354224848179261915075"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibonacci(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=98, misses=101, maxsize=128, currsize=101)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibonacci.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cache can be cleared with the cache_clear method\n",
    "fibonacci.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=0, misses=0, maxsize=128, currsize=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibonacci.cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cache_info()` method of a cached function using `@lru_cache` from Python's `functools` module returns a named tuple, `CacheInfo`, containing statistics about the performance of the cache. Here's what each of the fields in `CacheInfo(hits=9, misses=11, maxsize=None, currsize=11)` as an example represent:\n",
    "\n",
    "1. `hits`: The number of times a cache hit occurred. A cache hit is when the requested data is found in the cache, meaning the function does not need to execute because the result is already stored and can be returned immediately. In this case, there have been 9 cache hits.\n",
    "\n",
    "2. `misses`: The number of times a cache miss occurred. A cache miss is when the requested data is not found in the cache, requiring the function to execute and compute the result, which is then stored in the cache for future use. In this case, there have been 11 cache misses.\n",
    "\n",
    "3. `maxsize`: The maximum size of the cache, as defined by the `maxsize` parameter of the `@lru_cache` decorator. If `maxsize` is set to `None`, the cache can grow without bound. In this case, `maxsize` is `None`, meaning there is no fixed limit to the number of entries that the cache can hold.\n",
    "\n",
    "4. `currsize`: The current size of the cache, i.e., the number of entries currently stored in the cache. This number will always be less than or equal to `maxsize` if a `maxsize` is set. In this case, the current size of the cache is 11, which means there are 11 unique entries that have been stored.\n",
    "\n",
    "The `cache_info` statistics can be useful to understand and optimize the performance of the caching mechanism. High numbers of hits indicate that the cache is being effectively used, while a high number of misses may suggest that the cache size (`maxsize`) could be increased (if not `None`), or that the function's inputs are too varied and unique for the cache to be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Implementing Cache with TTL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you may want cached results to expire after a certain period. This is not directly supported by `@cache` or `@lru_cache`, but you can use third-party libraries or implement your own TTL cache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_'></a>[Using `cachetools` for TTL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cachetools` library provides a variety of cache implementations, including TTLCache, which supports time-to-live functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools in /Users/hejazizo/miniconda3/envs/py310/lib/python3.10/site-packages (5.3.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cachetools\n",
    "\n",
    "from cachetools import cached, TTLCache\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TTLCache instance with a TTL of 300 seconds\n",
    "ttl_cache = TTLCache(maxsize=100, ttl=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cached(ttl_cache)\n",
    "def get_data(key):\n",
    "    # Simulate a time-consuming data retrieval process\n",
    "    time.sleep(2)\n",
    "    return f\"Data for {key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data for key1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the cache as usual, knowing that entries will expire after the TTL\n",
    "get_data('key1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data for key1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data('key1')  # This call will retrieve the result from the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_'></a>[Building a Custom TTL Cache](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more hands-on approach, you can create a decorator that adds TTL to a simple caching mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttl_cache(ttl: int):\n",
    "    def decorator(func):\n",
    "        cache = {}\n",
    "        @functools.wraps(func)\n",
    "        def wrapped(*args, **kwargs):\n",
    "            nonlocal cache\n",
    "            current_time = time.time()\n",
    "            cache_key = args + tuple(sorted(kwargs.items()))\n",
    "\n",
    "            # Check cache for existing result and validity\n",
    "            if cache_key in cache:\n",
    "                result, timestamp = cache[cache_key]\n",
    "                if current_time - timestamp < ttl:\n",
    "                    return result\n",
    "\n",
    "            # Compute and cache new result\n",
    "            result = func(*args, **kwargs)\n",
    "            cache[cache_key] = (result, current_time)\n",
    "            return result\n",
    "        return wrapped\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "@ttl_cache(ttl=300)\n",
    "def get_data(key):\n",
    "    # Time-consuming computation\n",
    "    return f\"Data for {key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data for key1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cached result will expire after 300 seconds\n",
    "get_data('key1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we have explored various caching strategies and their implementations in Python. Caching is an essential tool in optimizing performance, and understanding these techniques will help you write more efficient and faster-running programs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
